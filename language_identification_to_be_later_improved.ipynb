{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1pRN16MxlAaELB-g_lQfx7QGEZdUOswr9",
      "authorship_tag": "ABX9TyN03yCxpG8TcWquz74N0Zx3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nirika-Lamichhane/Minor_Project-5-24-25-36-/blob/main/language_identification_to_be_later_improved.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VBX51vmwnWCv",
        "outputId": "d4464e0a-ae22-4eeb-82ee-d771ac85d2bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                text       label\n",
            "0                                Thumbnail is lit ðŸ˜‚ðŸ˜‚          EN\n",
            "1                  à¤®à¤¾à¤Ÿà¥‹à¤²à¤¾à¤ˆ à¤¹à¤¾à¤²à¥à¤¨à¥‡ à¤¹à¥‹ à¤œà¤¿à¤¤à¥‹à¤¸ à¤¯à¤¾ à¤¹à¤¾à¤°à¥‹à¤¸à¥      NE_DEV\n",
            "2          nicholousâŒ à¤šà¤¾à¤à¤¡à¥ˆ (à¤¨à¤¿à¤•à¥‹ à¤¹à¥‹à¤¸) mental cramps  CODE_MIXED\n",
            "3                                      He is right â¤          EN\n",
            "4  Twist ta Aayo ta kulman neskeya aaba raja aaun...      NE_ROM\n",
            "label\n",
            "EN            496\n",
            "NE_ROM        385\n",
            "NE_DEV        275\n",
            "CODE_MIXED    214\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "# Load your dataset\n",
        "df = pd.read_csv(\"/content/lang_identification_1.csv\")\n",
        "\n",
        "# Shuffle the dataset\n",
        "df = shuffle(df, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Save the shuffled dataset (optional)\n",
        "df.to_csv(\"shuffled_dataset.csv\", index=False)\n",
        "\n",
        "# Quick check\n",
        "print(df.head())\n",
        "print(df['label'].value_counts())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# Step 2: Load your shuffled dataset\n",
        "# Assuming your dataset is CSV with two columns: 'text' and 'label'\n",
        "df = pd.read_csv('/content/lang_identification_1.csv')  # replace with your CSV path\n",
        "\n",
        "texts = df['text'].astype(str).tolist()\n",
        "labels = df['label'].tolist()\n",
        "\n",
        "# Step 3: Split into training and testing sets (80%-20%)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    texts, labels, test_size=0.2, random_state=42, stratify=labels\n",
        ")\n",
        "\n",
        "# Step 4: TF-IDF vectorization\n",
        "tfidf_vectorizer = TfidfVectorizer(\n",
        "    analyzer='char_wb',   # character n-grams help for short Roman and noisy text\n",
        "    ngram_range=(2, 5),   # 2-5 char ngrams\n",
        "    lowercase=True,\n",
        "    max_features=10000    # limit features to keep it manageable\n",
        ")\n",
        "\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "# Step 5: Train SVM\n",
        "svm_clf = SVC(\n",
        "    kernel='linear',   # linear kernel works well for text\n",
        "    C=1.0,             # regularization parameter\n",
        "    probability=True,  # allows probability output if needed\n",
        "    class_weight='balanced',  # handle class imbalance\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "svm_clf.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Step 6: Evaluate\n",
        "y_pred = svm_clf.predict(X_test_tfidf)\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "vLkR-5cNpzHu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc058786-f36f-47bd-c2d5-cd11aa70e849"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8832116788321168\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "  CODE_MIXED       0.76      0.58      0.66        43\n",
            "          EN       0.87      0.97      0.92        99\n",
            "      NE_DEV       0.96      1.00      0.98        55\n",
            "      NE_ROM       0.89      0.86      0.87        77\n",
            "\n",
            "    accuracy                           0.88       274\n",
            "   macro avg       0.87      0.85      0.86       274\n",
            "weighted avg       0.88      0.88      0.88       274\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y numpy\n",
        "!pip install numpy==1.26.4\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 480
        },
        "id": "9yJUJngVJlFF",
        "outputId": "879f29a6-cad1-476a-8c7b-7174fa41d114"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: numpy 2.0.2\n",
            "Uninstalling numpy-2.0.2:\n",
            "  Successfully uninstalled numpy-2.0.2\n",
            "Collecting numpy==1.26.4\n",
            "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m86.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pytensor 2.36.3 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "rasterio 1.5.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "shap 0.50.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\n",
            "jaxlib 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "tobler 0.13.0 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "jax 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.26.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "780f6143054b4f79afd2f40e5a4daa23"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import fasttext\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 1. Load your dataset (CSV with columns: text, label)\n",
        "df = pd.read_csv(\"/content/lang_identification_1.csv\")\n",
        "\n",
        "# 2. Convert to FastText format\n",
        "# FastText expects each line as: __label__<label> <text>\n",
        "df['fasttext_format'] = \"__label__\" + df['label'] + \" \" + df['text']\n",
        "\n",
        "# 3. Split into training and validation sets\n",
        "train_texts, val_texts = train_test_split(df['fasttext_format'], test_size=0.2, random_state=42, shuffle=True)\n",
        "\n",
        "# 4. Save the datasets to temporary text files for FastText\n",
        "with open(\"train.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for line in train_texts:\n",
        "        f.write(line + \"\\n\")\n",
        "\n",
        "with open(\"valid.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for line in val_texts:\n",
        "        f.write(line + \"\\n\")\n",
        "\n",
        "# 5. Train the supervised FastText model\n",
        "model = fasttext.train_supervised(\n",
        "    input=\"train.txt\",\n",
        "    lr=0.5,              # learning rate, higher value for faster convergence\n",
        "    epoch=50,            # number of training iterations over the dataset\n",
        "    wordNgrams=2,        # use bigrams for capturing context (helps code-mixed)\n",
        "    dim=300,             # size of word vectors\n",
        "    minn=2, maxn=5,      # subword information for OOV words and Roman Nepali\n",
        "    bucket=200000,       # hash bucket size for subwords\n",
        "    loss='softmax',      # softmax is standard for multi-class classification\n",
        "    thread=4,            # number of CPU threads to use\n",
        ")\n",
        "\n",
        "# 6. Evaluate on validation set\n",
        "result = model.test(\"valid.txt\")\n",
        "print(\"FastText Validation Metrics:\")\n",
        "print(f\"Samples: {result[0]}, Precision: {result[1]:.4f}, Recall: {result[2]:.4f}\")\n",
        "\n",
        "# 7. Save the trained model for later use\n",
        "model.save_model(\"fasttext_language_model.bin\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OnFHWdlvKD0P",
        "outputId": "69c734d9-e9e2-4d62-e505-408a95c631eb"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FastText Validation Metrics:\n",
            "Samples: 274, Precision: 0.8905, Recall: 0.8905\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import fasttext\n",
        "import re\n",
        "import numpy as np\n",
        "\n",
        "# Load FastText model\n",
        "ft_model = fasttext.load_model(\"fasttext_language_model.bin\")\n",
        "\n",
        "# --- Helper: text cleaning ---\n",
        "def clean_text(text):\n",
        "    text = str(text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text\n",
        "\n",
        "# --- Word count ---\n",
        "def word_count(text):\n",
        "    return len(text.split())\n",
        "\n",
        "# --- SVM prediction ---\n",
        "def predict_svm(text):\n",
        "    text_tfidf = tfidf_vectorizer.transform([text])\n",
        "    pred = svm_clf.predict(text_tfidf)[0]\n",
        "    prob = max(svm_clf.predict_proba(text_tfidf)[0])\n",
        "    return pred, prob\n",
        "\n",
        "# --- FastText prediction ---\n",
        "def predict_fasttext(text):\n",
        "    labels, probs = ft_model.predict(text)\n",
        "\n",
        "    # Force safe conversion (NumPy 2.x compatible)\n",
        "    labels = list(labels)\n",
        "    probs = np.asarray(probs)\n",
        "\n",
        "    label = labels[0].replace(\"__label__\", \"\")\n",
        "    confidence = float(probs[0])\n",
        "\n",
        "    return label, confidence\n",
        "\n",
        "# --- Hybrid prediction ---\n",
        "\n",
        "CONFIDENCE_THRESHOLD = 0.57\n",
        "def hybrid_language_detect(text, threshold=6):\n",
        "    text = clean_text(text)\n",
        "    wc = word_count(text)\n",
        "\n",
        "    if wc <= threshold:\n",
        "        label, confidence = predict_svm(text)\n",
        "        source = \"SVM\"\n",
        "    else:\n",
        "        label, confidence = predict_fasttext(text)\n",
        "        source = \"FastText\"\n",
        "\n",
        "    if confidence < CONFIDENCE_THRESHOLD or wc < 2:\n",
        "        return {\n",
        "            \"text\": text,\n",
        "            \"predicted_label\": \"UNKNOWN\",\n",
        "            \"confidence\": round(float(confidence), 4),\n",
        "            \"model_used\": source,\n",
        "            \"word_count\": wc\n",
        "        }\n",
        "\n",
        "    return {\n",
        "        \"text\": text,\n",
        "        \"predicted_label\": label,\n",
        "        \"confidence\": round(confidence, 4),\n",
        "        \"model_used\": source,\n",
        "        \"word_count\": wc\n",
        "    }\n"
      ],
      "metadata": {
        "id": "QDwL5qc4Q4V1"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "examples = [\n",
        "    \"kp chor ðŸ‘\",\n",
        "    \"à¤¸à¤¬à¥ˆ à¤µà¥à¤¯à¤•à¥à¤¤à¤¿à¤¹à¤°à¥‚ à¤œà¤¨à¥à¤®à¤œà¤¾à¤¤ à¤¸à¥à¤µà¤¤à¤¨à¥à¤¤à¥à¤° à¤¹à¥à¤¨à¥\",\n",
        "    \"lau yaar not true\",\n",
        "    \"vote for balen now or never ðŸ‡³ðŸ‡µ\",\n",
        "    \"Jhapa ma aunu parcha\",\n",
        "    \"xya\"\n",
        "]\n",
        "\n",
        "for e in examples:\n",
        "    print(hybrid_language_detect(e))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UXwcwBVBR_B4",
        "outputId": "1fc0635e-7b87-4137-86bf-4af136ac6e62"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'text': 'kp chor ðŸ‘', 'predicted_label': 'EN', 'confidence': 0.6531, 'model_used': 'SVM', 'word_count': 3}\n",
            "{'text': 'à¤¸à¤¬à¥ˆ à¤µà¥à¤¯à¤•à¥à¤¤à¤¿à¤¹à¤°à¥‚ à¤œà¤¨à¥à¤®à¤œà¤¾à¤¤ à¤¸à¥à¤µà¤¤à¤¨à¥à¤¤à¥à¤° à¤¹à¥à¤¨à¥', 'predicted_label': 'NE_DEV', 'confidence': 0.9779, 'model_used': 'SVM', 'word_count': 5}\n",
            "{'text': 'lau yaar not true', 'predicted_label': 'CODE_MIXED', 'confidence': 0.7713, 'model_used': 'SVM', 'word_count': 4}\n",
            "{'text': 'vote for balen now or never ðŸ‡³ðŸ‡µ', 'predicted_label': 'EN', 'confidence': 0.9994, 'model_used': 'FastText', 'word_count': 7}\n",
            "{'text': 'Jhapa ma aunu parcha', 'predicted_label': 'NE_ROM', 'confidence': 0.891, 'model_used': 'SVM', 'word_count': 4}\n",
            "{'text': 'xya', 'predicted_label': 'UNKNOWN', 'confidence': 0.736, 'model_used': 'SVM', 'word_count': 1}\n"
          ]
        }
      ]
    }
  ]
}